{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use a specific page to start, e.g., an artist page or a specific city's archive.\n",
    "# You will need to build logic later to iterate through many pages/cities.\n",
    "CONCERTARCHIVES_URL = \"https://www.concertarchives.org/cities/atlanta\"\n",
    "TOURDATESEARCH_URL = \"https://www.tourdatesearch.com/tour-dates/us\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23784754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_concertarchives(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Concert Archives.\"\"\"\n",
    "    print(f\"-> Scraping Concert Archives: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: This selector is based on common event listing practices and MUST BE VERIFIED.\n",
    "        # It assumes a common class wraps each event listing.\n",
    "        event_containers = soup.find_all('div', class_='event-list-item') # Placeholder Selector\n",
    "        \n",
    "        for container in event_containers:\n",
    "            # Extract Date (Often an h3 or span)\n",
    "            date_element = container.find('time', class_='event-date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            # Extract Headliners/Artists (Often an anchor tag <a> in a prominent header)\n",
    "            artist_element = container.find('h4', class_='artist-name') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            # Extract Venue (Often a span or small text below the artist)\n",
    "            venue_element = container.find('a', class_='venue-name') # Placeholder Selector\n",
    "            venue = venue_element.text.strip() if venue_element else None\n",
    "            \n",
    "            if artist and date and venue:\n",
    "                events.append({\n",
    "                    \"source\": \"concertarchives\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": url.split('/')[-1] # Basic way to get city from URL\n",
    "                })\n",
    "        \n",
    "        print(f\"-> Found {len(events)} events on Concert Archives.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Concert Archives: {e}\")\n",
    "        \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tourdatesearch(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Tour Date Search.\"\"\"\n",
    "    print(f\"-> Scraping Tour Date Search: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: This selector is a general placeholder and MUST BE VERIFIED.\n",
    "        tour_containers = soup.find_all('div', class_='tour-listing') # Placeholder Selector\n",
    "        \n",
    "        for container in tour_containers:\n",
    "            # Extract Date\n",
    "            date_element = container.find('span', class_='date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            # Extract Artist\n",
    "            artist_element = container.find('h3', class_='tour-artist') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            # Extract Venue/City\n",
    "            location_element = container.find('span', class_='tour-location') # Placeholder Selector\n",
    "            location = location_element.text.strip() if location_element else None\n",
    "            \n",
    "            if artist and date and location:\n",
    "                # Simple split to separate city and venue (may require refinement)\n",
    "                location_parts = location.split(' at ')\n",
    "                venue = location_parts[1].strip() if len(location_parts) > 1 else location\n",
    "                city = location_parts[0].strip() if location_parts else location\n",
    "                \n",
    "                events.append({\n",
    "                    \"source\": \"tourdatesearch\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": city\n",
    "                })\n",
    "\n",
    "        print(f\"-> Found {len(events)} events on Tour Date Search.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Tour Date Search: {e}\")\n",
    "        \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Execute scrapes\n",
    "    all_concert_archives = scrape_concertarchives(CONCERTARCHIVES_URL)\n",
    "    time.sleep(15) # Be respectful: wait before hitting the next site\n",
    "    all_tour_dates = scrape_tourdatesearch(TOURDATESEARCH_URL)\n",
    "\n",
    "    # 2. Consolidate and Clean\n",
    "    all_events_raw = all_concert_archives + all_tour_dates\n",
    "    df_features = pd.DataFrame(all_events_raw)\n",
    "    \n",
    "    # 3. Create the unique identifier (Join Key)\n",
    "    # The join key must be consistent! Use Artist + Date + Venue as a unique identifier.\n",
    "    df_features['join_key'] = (\n",
    "        df_features['artist_name'].str.lower().str.replace('[^a-z0-9]', '', regex=True) + '_' +\n",
    "        df_features['event_date'].str.replace('[^a-z0-9]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 4. Save to Parquet\n",
    "    df_features.to_parquet(\"data/processed/event_features_raw.parquet\", index=False)\n",
    "    \n",
    "    print(\"\\n--- Feature Data Consolidation Complete ---\")\n",
    "    print(f\"Total unique events found: {df_features['join_key'].nunique()}\")\n",
    "    print(\"Data saved to data/processed/event_features_raw.parquet\")\n",
    "    print(df_features.head().to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
