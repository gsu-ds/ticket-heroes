{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use a specific page to start, e.g., an artist page or a specific city's archive.\n",
    "# You will need to build logic later to iterate through many pages/cities.\n",
    "CONCERTARCHIVES_URL = \"https://www.concertarchives.org/cities/atlanta\"\n",
    "TOURDATESEARCH_URL = \"https://www.tourdatesearch.com/tour-dates/us\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23784754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_concertarchives(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Concert Archives.\"\"\"\n",
    "    print(f\"-> Scraping Concert Archives: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: This selector is based on common event listing practices and MUST BE VERIFIED.\n",
    "        # It assumes a common class wraps each event listing.\n",
    "        event_containers = soup.find_all('div', class_='event-list-item') # Placeholder Selector\n",
    "        \n",
    "        for container in event_containers:\n",
    "            # Extract Date (Often an h3 or span)\n",
    "            date_element = container.find('time', class_='event-date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            # Extract Headliners/Artists (Often an anchor tag <a> in a prominent header)\n",
    "            artist_element = container.find('h4', class_='artist-name') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            # Extract Venue (Often a span or small text below the artist)\n",
    "            venue_element = container.find('a', class_='venue-name') # Placeholder Selector\n",
    "            venue = venue_element.text.strip() if venue_element else None\n",
    "            \n",
    "            if artist and date and venue:\n",
    "                events.append({\n",
    "                    \"source\": \"concertarchives\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": url.split('/')[-1] # Basic way to get city from URL\n",
    "                })\n",
    "        \n",
    "        print(f\"-> Found {len(events)} events on Concert Archives.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Concert Archives: {e}\")\n",
    "        \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tourdatesearch(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Tour Date Search.\"\"\"\n",
    "    print(f\"-> Scraping Tour Date Search: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: This selector is a general placeholder and MUST BE VERIFIED.\n",
    "        tour_containers = soup.find_all('div', class_='tour-listing') # Placeholder Selector\n",
    "        \n",
    "        for container in tour_containers:\n",
    "            # Extract Date\n",
    "            date_element = container.find('span', class_='date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            # Extract Artist\n",
    "            artist_element = container.find('h3', class_='tour-artist') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            # Extract Venue/City\n",
    "            location_element = container.find('span', class_='tour-location') # Placeholder Selector\n",
    "            location = location_element.text.strip() if location_element else None\n",
    "            \n",
    "            if artist and date and location:\n",
    "                # Simple split to separate city and venue (may require refinement)\n",
    "                location_parts = location.split(' at ')\n",
    "                venue = location_parts[1].strip() if len(location_parts) > 1 else location\n",
    "                city = location_parts[0].strip() if location_parts else location\n",
    "                \n",
    "                events.append({\n",
    "                    \"source\": \"tourdatesearch\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": city\n",
    "                })\n",
    "\n",
    "        print(f\"-> Found {len(events)} events on Tour Date Search.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Tour Date Search: {e}\")\n",
    "        \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Execute scrapes\n",
    "    all_concert_archives = scrape_concertarchives(CONCERTARCHIVES_URL)\n",
    "    time.sleep(15) # Be respectful: wait before hitting the next site\n",
    "    all_tour_dates = scrape_tourdatesearch(TOURDATESEARCH_URL)\n",
    "\n",
    "    # 2. Consolidate and Clean\n",
    "    all_events_raw = all_concert_archives + all_tour_dates\n",
    "    all_concert_archives = scrape_concertarchives(CONCERTARCHIVES_URL)\n",
    "    all_tour_dates = scrape_tourdatesearch(TOURDATESEARCH_URL)\n",
    "\n",
    "    all_events_raw = all_concert_archives + all_tour_dates\n",
    "\n",
    "if not all_events_raw:\n",
    "    print(\"WARNING: all_events_raw is empty. Check your URLs and selectors.\")\n",
    "else:\n",
    "    # Inspect the keys of the first dictionary\n",
    "    print(\"Keys found in first event dictionary:\", all_events_raw[0].keys())\n",
    "    df_features = pd.DataFrame(all_events_raw)\n",
    "    \n",
    "    # 3. Create the unique identifier (Join Key)\n",
    "    # The join key must be consistent! Use Artist + Date + Venue as a unique identifier.\n",
    "    df_features['join_key'] = (\n",
    "        df_features['artist_name'].str.lower().str.replace('[^a-z0-9]', '', regex=True) + '_' +\n",
    "        df_features['event_date'].str.replace('[^a-z0-9]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 4. Save to Parquet\n",
    "    df_features.to_parquet(\"data/processed/event_features_raw.parquet\", index=False)\n",
    "    \n",
    "    print(\"\\n--- Feature Data Consolidation Complete ---\")\n",
    "    print(f\"Total unique events found: {df_features['join_key'].nunique()}\")\n",
    "    print(\"Data saved to data/processed/event_features_raw.parquet\")\n",
    "    print(df_features.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "CONCERTARCHIVES_URL = \"https://www.concertarchives.org/cities/atlanta\"\n",
    "TOURDATESEARCH_URL = \"https://www.tourdatesearch.com/tour-dates/us\"\n",
    "\n",
    "# FIX: Robust Headers to bypass 403 Forbidden Error\n",
    "HEADERS = {\n",
    "    # Most Critical: Mimics a real Chrome browser on Windows\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36',\n",
    "    # Tells the server we accept HTML\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    # Pretends the request came from a search engine\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfadb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_concertarchives(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Concert Archives.\"\"\"\n",
    "    print(f\"-> Scraping Concert Archives: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        # FIX: Implement a delay to prevent immediate rate-limiting/banning\n",
    "        time.sleep(10) \n",
    "        \n",
    "        # FIX: Pass the new robust HEADERS dictionary\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status() # This will raise an error if the status is still 403\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: Selectors MUST BE VERIFIED against the live HTML.\n",
    "        event_containers = soup.find_all('div', class_='event-list-item') # Placeholder Selector\n",
    "        \n",
    "        for container in event_containers:\n",
    "            date_element = container.find('time', class_='event-date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            artist_element = container.find('h4', class_='artist-name') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            venue_element = container.find('a', class_='venue-name') # Placeholder Selector\n",
    "            venue = venue_element.text.strip() if venue_element else None\n",
    "            \n",
    "            if artist and date and venue:\n",
    "                events.append({\n",
    "                    \"source\": \"concertarchives\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": url.split('/')[-1]\n",
    "                })\n",
    "        \n",
    "        print(f\"-> Found {len(events)} events on Concert Archives.\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"!!! Error scraping Concert Archives: Failed to access URL ({e})\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Concert Archives: Parsing failed ({e})\")\n",
    "        \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tourdatesearch(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Tour Date Search.\"\"\"\n",
    "    print(f\"-> Scraping Tour Date Search: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        # We assume TourDateSearch is less strict, but still use the good headers\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: Selectors MUST BE VERIFIED against the live HTML.\n",
    "        tour_containers = soup.find_all('div', class_='tour-listing') # Placeholder Selector\n",
    "        \n",
    "        for container in tour_containers:\n",
    "            date_element = container.find('span', class_='date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            artist_element = container.find('h3', class_='tour-artist') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            location_element = container.find('span', class_='tour-location') # Placeholder Selector\n",
    "            location = location_element.text.strip() if location_element else None\n",
    "            \n",
    "            if artist and date and location:\n",
    "                # Simple split to separate city and venue (may require refinement)\n",
    "                location_parts = location.split(' at ')\n",
    "                venue = location_parts[1].strip() if len(location_parts) > 1 else location\n",
    "                city = location_parts[0].strip() if location_parts else location\n",
    "                \n",
    "                events.append({\n",
    "                    \"source\": \"tourdatesearch\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": city\n",
    "                })\n",
    "\n",
    "        print(f\"-> Found {len(events)} events on Tour Date Search.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Tour Date Search: {e}\")\n",
    "        \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Execute scrapes\n",
    "    # If the 403 persists, your IP may be temporarily blocked. Wait 15-30 minutes and try again.\n",
    "    all_concert_archives = scrape_concertarchives(CONCERTARCHIVES_URL)\n",
    "    time.sleep(15) # Pause before hitting the next site\n",
    "    all_tour_dates = scrape_tourdatesearch(TOURDATESEARCH_URL)\n",
    "\n",
    "    # 2. Consolidate and Clean\n",
    "    all_events_raw = all_concert_archives + all_tour_dates\n",
    "    df_features = pd.DataFrame(all_events_raw)\n",
    "    \n",
    "    # Check for empty data before creating join key (prevents KeyError)\n",
    "    if df_features.empty:\n",
    "        print(\"\\n--- WARNING: No data collected from either source. Cannot create join_key. ---\")\n",
    "        exit()\n",
    "        \n",
    "    # 3. Create the unique identifier (Join Key)\n",
    "    # This key is vital for linking to your StubHub price data.\n",
    "    df_features['join_key'] = (\n",
    "        df_features['artist_name'].str.lower().str.replace('[^a-z0-9]', '', regex=True) + '_' +\n",
    "        df_features['event_date'].str.replace('[^a-z0-9]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 4. Save to Parquet\n",
    "    df_features.to_parquet(\"data/processed/event_features_raw.parquet\", index=False)\n",
    "    \n",
    "    print(\"\\n--- Feature Data Consolidation Complete ---\")\n",
    "    print(f\"Total unique events found: {df_features['join_key'].nunique()}\")\n",
    "    print(\"Data saved to data/processed/event_features_raw.parquet\")\n",
    "    print(df_features.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f825578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# --- Playwright Configuration ---\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# --- Global Configuration ---\n",
    "CONCERTARCHIVES_URL = \"https://www.concertarchives.org/cities/atlanta\"\n",
    "TOURDATESEARCH_URL = \"https://www.tourdatesearch.com/tour-dates/us\"\n",
    "\n",
    "# Headers are still used by the synchronous scraper and Playwright's page setup\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36',\n",
    "} \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ASYNC SCRAPING FUNCTION (Concert Archives - Uses Playwright)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "async def scrape_concertarchives_async(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrapes event metadata from Concert Archives using Playwright to bypass 403/JS checks.\n",
    "    \"\"\"\n",
    "    print(f\"-> Scraping Concert Archives (ASYNC Playwright): {url}\")\n",
    "    events = []\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        # Launch a headless Chromium browser\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page(extra_http_headers=HEADERS)\n",
    "\n",
    "        try:\n",
    "            # Navigate to the URL and wait until the network is mostly idle (page loaded)\n",
    "            await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
    "            await asyncio.sleep(5) # Give extra time for rendering/JS checks\n",
    "            \n",
    "            # Get the fully rendered HTML content\n",
    "            content = await page.content()\n",
    "            \n",
    "            # --- BeautifulSoup Parsing ---\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # NOTE: Selectors MUST BE VERIFIED against the live HTML.\n",
    "            event_containers = soup.find_all('div', class_='event-list-item') # Placeholder Selector\n",
    "            \n",
    "            for container in event_containers:\n",
    "                date_element = container.find('time', class_='event-date') # Placeholder Selector\n",
    "                date = date_element.text.strip() if date_element else None\n",
    "                \n",
    "                artist_element = container.find('h4', class_='artist-name') # Placeholder Selector\n",
    "                artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "                venue_element = container.find('a', class_='venue-name') # Placeholder Selector\n",
    "                venue = venue_element.text.strip() if venue_element else None\n",
    "                \n",
    "                if artist and date and venue:\n",
    "                    events.append({\n",
    "                        \"source\": \"concertarchives\",\n",
    "                        \"artist_name\": artist,\n",
    "                        \"event_date\": date,\n",
    "                        \"venue_name\": venue,\n",
    "                        \"city\": url.split('/')[-1]\n",
    "                    })\n",
    "            \n",
    "            print(f\"-> Found {len(events)} events on Concert Archives.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error scraping Concert Archives (Playwright): {e}\")\n",
    "\n",
    "        finally:\n",
    "            await browser.close()\n",
    "            \n",
    "    return events\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# SYNCHRONOUS SCRAPING FUNCTION (Tour Date Search - Uses Requests)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def scrape_tourdatesearch_sync(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Tour Date Search using simple requests.\"\"\"\n",
    "    print(f\"-> Scraping Tour Date Search (SYNC Requests): {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # NOTE: Selectors MUST BE VERIFIED against the live HTML.\n",
    "        tour_containers = soup.find_all('div', class_='tour-listing') # Placeholder Selector\n",
    "        \n",
    "        for container in tour_containers:\n",
    "            date_element = container.find('span', class_='date') # Placeholder Selector\n",
    "            date = date_element.text.strip() if date_element else None\n",
    "            \n",
    "            artist_element = container.find('h3', class_='tour-artist') # Placeholder Selector\n",
    "            artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "            location_element = container.find('span', class_='tour-location') # Placeholder Selector\n",
    "            location = location_element.text.strip() if location_element else None\n",
    "            \n",
    "            if artist and date and location:\n",
    "                location_parts = location.split(' at ')\n",
    "                venue = location_parts[1].strip() if len(location_parts) > 1 else location\n",
    "                city = location_parts[0].strip() if location_parts else location\n",
    "                \n",
    "                events.append({\n",
    "                    \"source\": \"tourdatesearch\",\n",
    "                    \"artist_name\": artist,\n",
    "                    \"event_date\": date,\n",
    "                    \"venue_name\": venue,\n",
    "                    \"city\": city\n",
    "                })\n",
    "\n",
    "        print(f\"-> Found {len(events)} events on Tour Date Search.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Tour Date Search: {e}\")\n",
    "        \n",
    "    return events\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN EXECUTION BLOCK (Uses asyncio.run to call the async function)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "async def main_scraper_task():\n",
    "    # 1. Execute async scrape for Concert Archives\n",
    "    all_concert_archives = await scrape_concertarchives_async(CONCERTARCHIVES_URL)\n",
    "    \n",
    "    # 2. Execute sync scrape for Tour Date Search\n",
    "    time.sleep(5) # Pause before hitting the next site\n",
    "    all_tour_dates = scrape_tourdatesearch_sync(TOURDATESEARCH_URL)\n",
    "\n",
    "    # 3. Consolidate and Clean\n",
    "    all_events_raw = all_concert_archives + all_tour_dates\n",
    "    df_features = pd.DataFrame(all_events_raw)\n",
    "    \n",
    "    # Check for empty data before creating join key (prevents KeyError)\n",
    "    if df_features.empty:\n",
    "        print(\"\\n--- WARNING: No data collected from either source. Aborting save. ---\")\n",
    "        return pd.DataFrame() \n",
    "        \n",
    "    # 4. Create the unique identifier (Join Key)\n",
    "    # This key is vital for linking to your StubHub price data.\n",
    "    df_features['join_key'] = (\n",
    "        df_features['artist_name'].str.lower().str.replace('[^a-z0-9]', '', regex=True) + '_' +\n",
    "        df_features['event_date'].str.replace('[^a-z0-9]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 5. Save to Parquet\n",
    "    df_features.to_parquet(\"data/processed/event_features_raw.parquet\", index=False)\n",
    "    \n",
    "    print(\"\\n--- Feature Data Consolidation Complete ---\")\n",
    "    print(f\"Total unique events found: {df_features['join_key'].nunique()}\")\n",
    "    print(\"Data saved to data/processed/event_features_raw.parquet\")\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc313e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTE IN A NEW CELL ---\n",
    "\n",
    "# Call the function directly using await\n",
    "df_final = await main_scraper_task()\n",
    "\n",
    "if not df_final.empty:\n",
    "    print(\"\\n--- FINAL FEATURE DATASET SAMPLE ---\")\n",
    "    print(df_final.head().to_markdown())\n",
    "else:\n",
    "    print(\"\\n--- ASYNC PIPELINE RAN, BUT RETURNED NO DATA ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# --- Playwright Configuration ---\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# --- Global Configuration (Define your target URLs) ---\n",
    "# CORRECTED URL for Concert Archives\n",
    "CONCERTARCHIVES_URL = \"https://www.concertarchives.org/locations/atlanta-ga\"\n",
    "# CORRECTED URL for Tour Date Search (Base domain to avoid 404)\n",
    "TOURDATESEARCH_URL = \"https://www.tourdatesearch.com/\" \n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36',\n",
    "} \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ASYNC SCRAPER 1: Concert Archives (Bypasses 403 using Playwright and Corrected Selectors)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "async def scrape_concertarchives_async(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrapes event metadata from Concert Archives using Playwright and CORRECTED selectors\n",
    "    for the HTML table structure.\n",
    "    \"\"\"\n",
    "    print(f\"-> Scraping Concert Archives: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page(extra_http_headers=HEADERS)\n",
    "\n",
    "        try:\n",
    "            # Bypass 403/JS checks\n",
    "            await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
    "            await asyncio.sleep(5) \n",
    "            content = await page.content()\n",
    "            \n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # **CORRECTED SELECTOR 1: Targets table rows inside the concert listing table**\n",
    "            event_rows = soup.select('table#band-show-table tbody tr') \n",
    "            \n",
    "            if not event_rows:\n",
    "                print(\"-> WARNING: Concert Archives table found but contains 0 event rows.\")\n",
    "                \n",
    "            for row in event_rows:\n",
    "                # Use .select_one for precision within the row\n",
    "                \n",
    "                # CORRECTED SELECTOR 2: Date (first column TD, span)\n",
    "                date_element = row.select_one('td:nth-of-type(1) span')\n",
    "                date = date_element.text.strip() if date_element else None\n",
    "                \n",
    "                # CORRECTED SELECTOR 3: Artist (second column TD, strong a)\n",
    "                artist_element = row.select_one('td:nth-of-type(2) strong a') \n",
    "                artist = artist_element.text.strip() if artist_element else None\n",
    "\n",
    "                # CORRECTED SELECTOR 4: Venue (third column TD, a link)\n",
    "                venue_element = row.select_one('td:nth-of-type(3) a')\n",
    "                venue = venue_element.text.strip() if venue_element else None\n",
    "                \n",
    "                if artist and date and venue:\n",
    "                    # Skip the first cancelled event which still shows up\n",
    "                    if date.startswith('Mar 10, 2027') and 'Cancelled' in row.text:\n",
    "                         continue\n",
    "                         \n",
    "                    events.append({\n",
    "                        \"source\": \"concertarchives\",\n",
    "                        \"artist_name\": artist,\n",
    "                        \"event_date\": date,\n",
    "                        \"venue_name\": venue,\n",
    "                        \"city\": url.split('/')[-1] # Extracts 'atlanta-ga'\n",
    "                    })\n",
    "            \n",
    "            print(f\"-> Found {len(events)} events on Concert Archives.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Fatal Error scraping Concert Archives: {e}\")\n",
    "\n",
    "        finally:\n",
    "            await browser.close()\n",
    "            \n",
    "    return events\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# SYNCHRONOUS SCRAPER 2: Tour Date Search (Corrected Selectors for div structure)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def scrape_tourdatesearch_sync(url: str) -> List[Dict]:\n",
    "    \"\"\"Scrapes event metadata from Tour Date Search using simple requests and CORRECTED selectors.\"\"\"\n",
    "    print(f\"-> Scraping Tour Date Search: {url}\")\n",
    "    events = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 1. Target the main results container\n",
    "        results_container = soup.find('div', id='results')\n",
    "        if not results_container:\n",
    "            print(\"-> WARNING: Could not find main results container #results.\")\n",
    "            return events\n",
    "\n",
    "        # **CORRECTED SELECTOR 1: Find all event rows**\n",
    "        event_containers = results_container.find_all('div', class_='searchrow') \n",
    "        \n",
    "        for container in event_containers:\n",
    "            # 2. Extract Date components\n",
    "            date_wrapper = container.find('div', class_='datewrapper')\n",
    "            if not date_wrapper: continue\n",
    "\n",
    "            # Combine date parts into a standard string\n",
    "            dow = date_wrapper.find('div', class_='dow').text.strip()\n",
    "            day = date_wrapper.find('div', class_='day').text.strip()\n",
    "            year = date_wrapper.find('div', class_='year').text.strip()\n",
    "            date = f\"{dow}, {day} {year}\"\n",
    "            \n",
    "            # 3. Extract Artist and Venue/Location\n",
    "            venue_col = container.find('div', class_='vcol')\n",
    "            if not venue_col: continue\n",
    "            \n",
    "            # CORRECTED SELECTOR 2: Artist Name (inside div.venue b)\n",
    "            artist_bold = venue_col.find('div', class_='venue').find('b')\n",
    "            artist = artist_bold.text.strip() if artist_bold else None\n",
    "\n",
    "            if not artist: continue\n",
    "            \n",
    "            # Full text structure is \"<b>Artist</b> at Venue in City, ST\"\n",
    "            venue_text = venue_col.find('div', class_='venue').text.strip()\n",
    "            clean_text = venue_text.replace(artist, '').strip() \n",
    "            \n",
    "            # Parse Venue and City from the remaining text\n",
    "            try:\n",
    "                # Looks for \"at [Venue] in [City, ST]\"\n",
    "                venue_city_raw = clean_text.split(' at ', 1)[1]\n",
    "                venue_name = venue_city_raw.split(' in ', 1)[0].strip()\n",
    "                location_full = venue_city_raw.split(' in ', 1)[1].strip()\n",
    "            except IndexError:\n",
    "                venue_name = \"Unknown\"\n",
    "                location_full = \"Unknown\"\n",
    "\n",
    "            events.append({\n",
    "                \"source\": \"tourdatesearch\",\n",
    "                \"artist_name\": artist,\n",
    "                \"event_date\": date,\n",
    "                \"venue_name\": venue_name,\n",
    "                \"city\": location_full\n",
    "            })\n",
    "\n",
    "        print(f\"-> Found {len(events)} events on Tour Date Search.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error scraping Tour Date Search: {e}\")\n",
    "        \n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN EXECUTION BLOCK (Entry point for Notebook execution)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "async def main_scraper_task():\n",
    "    print(\"\\n--- STARTING ASYNC FEATURE SCRAPING PIPELINE ---\")\n",
    "    \n",
    "    # 1. Execute async scrape for Concert Archives\n",
    "    all_concert_archives = await scrape_concertarchives_async(CONCERTARCHIVES_URL)\n",
    "    \n",
    "    # 2. Execute sync scrape for Tour Date Search\n",
    "    time.sleep(5) \n",
    "    all_tour_dates = scrape_tourdatesearch_sync(TOURDATESEARCH_URL)\n",
    "\n",
    "    # 3. Consolidate and Clean\n",
    "    all_events_raw = all_concert_archives + all_tour_dates\n",
    "    df_features = pd.DataFrame(all_events_raw)\n",
    "    \n",
    "    if df_features.empty:\n",
    "        print(\"\\n--- WARNING: No data collected. Aborting save. ---\")\n",
    "        return pd.DataFrame() \n",
    "        \n",
    "    # 4. Create the unique identifier (Join Key)\n",
    "    df_features['join_key'] = (\n",
    "        df_features['artist_name'].str.lower().str.replace('[^a-z0-9]', '', regex=True) + '_' +\n",
    "        df_features['event_date'].str.replace('[^a-z0-9]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 5. Save to Parquet\n",
    "    df_features.to_parquet(\"data/processed/event_features_raw.parquet\", index=False)\n",
    "    \n",
    "    print(\"\\n--- Feature Data Consolidation Complete ---\")\n",
    "    print(f\"Total unique events found: {df_features['join_key'].nunique()}\")\n",
    "    print(\"Data saved to data/processed/event_features_raw.parquet\")\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718669ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# \n",
    "# Execute the following line in a new cell to run the entire pipeline:\n",
    "\n",
    "df_final = await main_scraper_task()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
