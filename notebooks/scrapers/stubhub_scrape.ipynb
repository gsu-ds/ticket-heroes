{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d69aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Starting scrape for event: Your-Concert-ID-from-Bandsintown\n",
      "!!! Error during WebDriver operation: Message: unknown error: cannot find Chrome binary\n",
      "Stacktrace:\n",
      "#0 0x610df43414e3 <unknown>\n",
      "#1 0x610df4070c76 <unknown>\n",
      "#2 0x610df4097757 <unknown>\n",
      "#3 0x610df4096029 <unknown>\n",
      "#4 0x610df40d4ccc <unknown>\n",
      "#5 0x610df40d447f <unknown>\n",
      "#6 0x610df40cbde3 <unknown>\n",
      "#7 0x610df40a12dd <unknown>\n",
      "#8 0x610df40a234e <unknown>\n",
      "#9 0x610df43013e4 <unknown>\n",
      "#10 0x610df43053d7 <unknown>\n",
      "#11 0x610df430fb20 <unknown>\n",
      "#12 0x610df4306023 <unknown>\n",
      "#13 0x610df42d41aa <unknown>\n",
      "#14 0x610df432a6b8 <unknown>\n",
      "#15 0x610df432a847 <unknown>\n",
      "#16 0x610df433a243 <unknown>\n",
      "#17 0x78c9d7ca9aa4 <unknown>\n",
      "#18 0x78c9d7d36c6c <unknown>\n",
      "\n",
      "\n",
      "--- Scrape Failed ---\n",
      "Check the URL and ensure the CSS selectors are correct.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- Configuration ---\n",
    "# REPLACE THIS URL WITH A REAL STUBHUB EVENT URL\n",
    "STUBHUB_EVENT_URL = \"YOUR_STUBHUB_EVENT_URL_HERE\" \n",
    "WAIT_TIME = 5 # Time (in seconds) to wait for JavaScript to load tickets\n",
    "\n",
    "def scrape_stubhub_tickets(url: str, event_id: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrapes ticket listings from a single StubHub event URL using Selenium.\n",
    "    \"\"\"\n",
    "    print(f\"-> Starting scrape for event: {event_id}\")\n",
    "    \n",
    "    # Initialize Selenium WebDriver (handles driver executable)\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        # Run in Headless mode for efficiency (no visible browser window)\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        \n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        # IMPORTANT: Wait for the JavaScript to execute and load all ticket listings\n",
    "        print(f\"   Waiting {WAIT_TIME} seconds for listings to load...\")\n",
    "        time.sleep(WAIT_TIME)\n",
    "        \n",
    "        # Get the fully rendered page source\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error during WebDriver operation: {e}\")\n",
    "        return []\n",
    "\n",
    "    # --- BeautifulSoup Parsing ---\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    ticket_listings = []\n",
    "    \n",
    "    # NOTE: You MUST inspect the live page to find the current, correct CSS selectors\n",
    "    # for ticket rows, prices, and sections.\n",
    "    # \n",
    "    \n",
    "    # Find all ticket listing containers (These are EXAMPLE class names)\n",
    "    listing_rows = soup.find_all('div', class_='listing-row') \n",
    "    \n",
    "    for row in listing_rows:\n",
    "        try:\n",
    "            # Extract Target Variable: Price\n",
    "            price_element = row.find('span', class_='price-value') # EXAMPLE SELECTOR\n",
    "            price = float(price_element.text.replace('$', '').replace(',', '').strip()) if price_element else None\n",
    "            \n",
    "            # Extract Features: Section and Row\n",
    "            section_element = row.find('div', class_='section-name') # EXAMPLE SELECTOR\n",
    "            section = section_element.text.strip() if section_element else None\n",
    "            \n",
    "            row_element = row.find('div', class_='row-name') # EXAMPLE SELECTOR\n",
    "            ticket_row = row_element.text.strip() if row_element else None\n",
    "            \n",
    "            ticket_listings.append({\n",
    "                \"event_id\": event_id,\n",
    "                \"extraction_time\": pd.Timestamp.now(),\n",
    "                \"ticket_price\": price,              # <-- Target Variable (Final Sale Price)\n",
    "                \"section\": section,                 # <-- Feature\n",
    "                \"ticket_row\": ticket_row,           # <-- Feature\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Skip invalid rows or rows with missing data\n",
    "            continue\n",
    "\n",
    "    print(f\"-> Successfully extracted {len(ticket_listings)} ticket listings.\")\n",
    "    return ticket_listings\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    EVENT_ID = \"Your-Concert-ID-from-Bandsintown\"\n",
    "    \n",
    "    all_data = scrape_stubhub_tickets(STUBHUB_EVENT_URL, EVENT_ID)\n",
    "    \n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        # Save to parquet to match your project's structure\n",
    "        df.to_parquet(\"data/processed/stubhub_ticket_panel.parquet\", index=False)\n",
    "        print(\"\\n--- Scrape Complete ---\")\n",
    "        print(f\"Data saved to data/processed/stubhub_ticket_panel.parquet\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"\\n--- Scrape Failed ---\")\n",
    "        print(\"Check the URL and ensure the CSS selectors are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StubHub recent\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "\n",
    "# Use undetected_chromedriver for stealth\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# --- Configuration ---\n",
    "# REPLACE THESE WITH YOUR DATA\n",
    "STUBHUB_EVENT_URL = \"YOUR_STUBHUB_EVENT_URL_HERE\" \n",
    "EVENT_ID = \"YOUR-EVENT-ID-HERE\" \n",
    "\n",
    "def scrape_stubhub_tickets_stealth(url: str, event_id: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrapes ticket listings from a single StubHub event URL using Undetected-Chromedriver.\n",
    "    \"\"\"\n",
    "    print(f\"-> Starting stealth scrape for event: {event_id}\")\n",
    "    \n",
    "    # Initialize Undetected-Chromedriver\n",
    "    options = uc.ChromeOptions()\n",
    "    # It's often better to try non-headless first to visually confirm it works\n",
    "    # options.add_argument('--headless') # Keep this commented out for initial testing\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    driver = uc.Chrome(options=options) \n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # CRITICAL STEP: Wait for the dynamic content (ticket listings) to appear.\n",
    "        # REPLACE 'TICKET_LISTING_CONTAINER_ID' with the actual unique ID or class of the main ticket section.\n",
    "        # Look for a unique data-qa attribute if available.\n",
    "        TICKET_LISTING_SELECTOR = (By.CSS_SELECTOR, 'div[data-qa=\"ticket-listing\"]') # PLACEHOLDER\n",
    "        \n",
    "        print(\"   Waiting for listings to load...\")\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located(TICKET_LISTING_SELECTOR))\n",
    "        \n",
    "        # Mimic human behavior with a small delay\n",
    "        time.sleep(2) \n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error during stealth scrape: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # --- BeautifulSoup Parsing ---\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    ticket_listings = []\n",
    "    \n",
    "    # YOU MUST REPLACE ALL SELECTORS BELOW with the actual classes from a live page inspection.\n",
    "    listing_rows = soup.find_all('div', class_='listing-row-class') # PLACEHOLDER for the entire ticket row\n",
    "    \n",
    "    for row in listing_rows:\n",
    "        try:\n",
    "            # Target Variable: Price\n",
    "            price_element = row.find('span', class_='price-value-class') # PLACEHOLDER\n",
    "            price = float(price_element.text.replace('$', '').replace(',', '').strip()) if price_element else None\n",
    "            \n",
    "            # Feature: Section and Row\n",
    "            section_element = row.find('div', class_='section-name-class') # PLACEHOLDER\n",
    "            section = section_element.text.strip() if section_element else None\n",
    "            \n",
    "            row_element = row.find('div', class_='row-name-class') # PLACEHOLDER\n",
    "            ticket_row = row_element.text.strip() if row_element else None\n",
    "            \n",
    "            # --- Data Point Extraction ---\n",
    "            ticket_listings.append({\n",
    "                \"event_id\": event_id,\n",
    "                \"extraction_time\": pd.Timestamp.now(),\n",
    "                \"ticket_price\": price,         \n",
    "                \"section\": section,            \n",
    "                \"ticket_row\": ticket_row,      \n",
    "            })\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"-> Successfully extracted {len(ticket_listings)} ticket listings.\")\n",
    "    return ticket_listings\n",
    "\n",
    "# --- Execution Block (You must use asyncio.run() or run in a notebook) ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    all_data = scrape_stubhub_tickets_stealth(STUBHUB_EVENT_URL, EVENT_ID)\n",
    "    \n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_parquet(\"data/processed/stubhub_ticket_panel.parquet\", index=False)\n",
    "        print(\"\\n--- Scrape Complete ---\")\n",
    "        print(df.head().to_markdown())\n",
    "    else:\n",
    "        print(\"\\n--- Scrape Failed ---\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
