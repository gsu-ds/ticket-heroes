{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55119fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import requests_cache\n",
    "import openmeteo_requests\n",
    "from retry_requests import retry\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Weather cache directory\n",
    "WEATHER_CACHE_DIR = Path(\"data/weather_cache\")\n",
    "WEATHER_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdfee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_locations(event_df: pd.DataFrame) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Returns list of unique (lat, lon) pairs.\n",
    "    Filters out missing or invalid coordinates.\n",
    "    \"\"\"\n",
    "    coords = (\n",
    "        event_df[[\"latitude\", \"longitude\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .itertuples(index=False, name=None)\n",
    "    )\n",
    "    return list(coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13caf628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_for_location(lat: float, lon: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch historical hourly + daily weather for a single location.\n",
    "    \"\"\"\n",
    "\n",
    "    console.print(\n",
    "        Panel(\n",
    "            f\"[bold cyan]Fetching Weather for Location[/bold cyan]\\nLat={lat:.3f}, Lon={lon:.3f}\",\n",
    "            border_style=\"cyan\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cache_session = requests_cache.CachedSession(\".weather_cache\", expire_after=-1)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.3)\n",
    "    client = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": START_DATE,\n",
    "        \"end_date\": END_DATE,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\",\n",
    "            \"precipitation\",\n",
    "            \"rain\",\n",
    "            \"apparent_temperature\",\n",
    "            \"weather_code\",\n",
    "            \"is_day\",\n",
    "        ],\n",
    "        \"daily\": [\n",
    "            \"sunrise\",\n",
    "            \"daylight_duration\",\n",
    "            \"sunshine_duration\",\n",
    "            \"precipitation_hours\",\n",
    "            \"rain_sum\",\n",
    "            \"temperature_2m_mean\",\n",
    "            \"weather_code\",\n",
    "        ],\n",
    "        \"timezone\": \"America/New_York\",\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "    }\n",
    "\n",
    "    responses = client.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # -------- HOURLY DATA --------\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame({\n",
    "        \"datetime\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\",\n",
    "        ),\n",
    "        \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "        \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "        \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "        \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "        \"weather_code_hourly\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "        \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "    })\n",
    "\n",
    "    hourly_df[\"datetime\"] = hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "\n",
    "    # -------- DAILY DATA --------\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame({\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "            inclusive=\"left\",\n",
    "        ),\n",
    "        \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "        \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "        \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "        \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "        \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "        \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "        \"weather_code_daily\": daily.Variables(6).ValuesAsNumpy(),\n",
    "    })\n",
    "\n",
    "    daily_df[\"date\"] = daily_df[\"date\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None).dt.date\n",
    "\n",
    "    return hourly_df, daily_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35274cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_fetch_weather(lat: float, lon: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    safe_lat = f\"{lat:.3f}\".replace(\".\", \"_\")\n",
    "    safe_lon = f\"{lon:.3f}\".replace(\".\", \"_\")\n",
    "    base = WEATHER_CACHE_DIR / f\"weather_{safe_lat}_{safe_lon}\"\n",
    "\n",
    "    hourly_file = base.with_suffix(\".hourly.parquet\")\n",
    "    daily_file = base.with_suffix(\".daily.parquet\")\n",
    "\n",
    "    if hourly_file.exists() and daily_file.exists():\n",
    "        console.print(f\"[green]Using cached weather for ({lat}, {lon})[/green]\")\n",
    "        return (\n",
    "            pd.read_parquet(hourly_file),\n",
    "            pd.read_parquet(daily_file)\n",
    "        )\n",
    "\n",
    "    hourly_df, daily_df = fetch_weather_for_location(lat, lon)\n",
    "\n",
    "    hourly_df.to_parquet(hourly_file, index=False)\n",
    "    daily_df.to_parquet(daily_file, index=False)\n",
    "\n",
    "    console.print(f\"[green]Cached weather for ({lat}, {lon})[/green]\")\n",
    "\n",
    "    return hourly_df, daily_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906a0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_for_all_event_locations(event_df: pd.DataFrame) -> Dict[tuple, dict]:\n",
    "    \"\"\"\n",
    "    Fetches or loads cached weather for all unique venue coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    locations = get_unique_locations(event_df)\n",
    "\n",
    "    console.print(\n",
    "        Panel(\n",
    "            f\"[cyan]Fetching Weather for {len(locations)} Unique Venue Locations[/cyan]\",\n",
    "            border_style=\"cyan\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    weather_map = {}\n",
    "\n",
    "    for lat, lon in locations:\n",
    "        hourly, daily = load_or_fetch_weather(lat, lon)\n",
    "        weather_map[(lat, lon)] = {\"hourly\": hourly, \"daily\": daily}\n",
    "\n",
    "    return weather_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3580b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_event_weather(event_df: pd.DataFrame, weather_map: Dict[tuple, dict]) -> pd.DataFrame:\n",
    "\n",
    "    df = event_df.copy()\n",
    "    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\n",
    "\n",
    "    df_weather = []\n",
    "\n",
    "    # Group events by location to speed up merging\n",
    "    for (lat, lon), group in df.groupby([\"latitude\", \"longitude\"]):\n",
    "\n",
    "        if (lat, lon) not in weather_map:\n",
    "            console.print(f\"[yellow]No weather found for ({lat}, {lon})[/yellow]\")\n",
    "            df_weather.append(group)\n",
    "            continue\n",
    "\n",
    "        hourly_w = weather_map[(lat, lon)][\"hourly\"].copy()\n",
    "        daily_w = weather_map[(lat, lon)][\"daily\"].copy()\n",
    "\n",
    "        hourly_w[\"weather_hour\"] = pd.to_datetime(hourly_w[\"datetime\"])\n",
    "        daily_w[\"weather_date\"] = pd.to_datetime(daily_w[\"date\"]).dt.date\n",
    "\n",
    "        temp = group.copy()\n",
    "\n",
    "        # HOURLY MERGE\n",
    "        temp[\"event_hour\"] = temp[\"event_date\"].dt.floor(\"h\")\n",
    "        temp = temp.merge(hourly_w.drop(columns=[\"datetime\"]), left_on=\"event_hour\", right_on=\"weather_hour\", how=\"left\")\n",
    "\n",
    "        # DAILY MERGE\n",
    "        temp[\"event_day\"] = temp[\"event_date\"].dt.date\n",
    "        temp = temp.merge(daily_w.drop(columns=[\"date\"]), left_on=\"event_day\", right_on=\"weather_date\", how=\"left\")\n",
    "\n",
    "        df_weather.append(temp)\n",
    "\n",
    "    merged = pd.concat(df_weather, ignore_index=True)\n",
    "\n",
    "    # Clean helper columns\n",
    "    merged.drop(columns=[\"event_hour\", \"weather_hour\", \"event_day\", \"weather_date\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    console.print(\"[green]Weather successfully merged into event data![/green]\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5d408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"temp_f\" not in df.columns:\n",
    "        console.print(\"[yellow]No temp_f column found — skipping flags.[/yellow]\")\n",
    "        return df\n",
    "\n",
    "    p85 = df[\"temp_f\"].dropna().quantile(0.85)\n",
    "    p15 = df[\"temp_f\"].dropna().quantile(0.15)\n",
    "\n",
    "    df[\"is_hot\"] = (df[\"temp_f\"] >= p85).astype(int)\n",
    "    df[\"is_cold\"] = (df[\"temp_f\"] <= p15).astype(int)\n",
    "    df[\"is_raining\"] = (df.get(\"precip_in\", 0) > 0.01).astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09dc7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weather_event_panel(event_json_path: str, out_path: str):\n",
    "    console.print(Panel(\"[bold cyan]Building Weather-Enriched Event Panel[/bold cyan]\"))\n",
    "\n",
    "    df_events = pd.read_json(event_json_path)\n",
    "\n",
    "    # Ensure numeric coords\n",
    "    df_events[\"latitude\"] = pd.to_numeric(df_events[\"latitude\"], errors=\"coerce\")\n",
    "    df_events[\"longitude\"] = pd.to_numeric(df_events[\"longitude\"], errors=\"coerce\")\n",
    "\n",
    "    console.print(f\"[blue]Loaded {len(df_events):,} events[/blue]\")\n",
    "\n",
    "    # Fetch + cache weather\n",
    "    weather_map = fetch_weather_for_all_event_locations(df_events)\n",
    "\n",
    "    # Merge weather into events\n",
    "    enriched = merge_event_weather(df_events, weather_map)\n",
    "\n",
    "    # Add flags\n",
    "    enriched = add_weather_flags(enriched)\n",
    "\n",
    "    # Save\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    enriched.to_parquet(out_path, index=False)\n",
    "\n",
    "    console.print(f\"[green]Saved enriched panel → {out_path}[/green]\")\n",
    "\n",
    "    return enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7442d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Building Weather-Enriched Event Panel</span>                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ \u001b[1;36mBuilding Weather-Enriched Event Panel\u001b[0m                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File data/raw/ticketmaster_us_music_latest.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m enriched_df = \u001b[43mbuild_weather_event_panel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevent_json_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/raw/ticketmaster_us_music_latest.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/processed/event_weather_panel.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m enriched_df.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mbuild_weather_event_panel\u001b[39m\u001b[34m(event_json_path, out_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_weather_event_panel\u001b[39m(event_json_path: \u001b[38;5;28mstr\u001b[39m, out_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      2\u001b[39m     console.print(Panel(\u001b[33m\"\u001b[39m\u001b[33m[bold cyan]Building Weather-Enriched Event Panel[/bold cyan]\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     df_events = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_json_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Ensure numeric coords\u001b[39;00m\n\u001b[32m      7\u001b[39m     df_events[\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_numeric(df_events[\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ticket-heroes/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:791\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient != \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m     convert_axes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m json_reader = \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ticket-heroes/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:904\u001b[39m, in \u001b[36mJsonReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = filepath_or_buffer\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mujson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._preprocess_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ticket-heroes/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:960\u001b[39m, in \u001b[36mJsonReader._get_data_from_filepath\u001b[39m\u001b[34m(self, filepath_or_buffer)\u001b[39m\n\u001b[32m    952\u001b[39m     filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    954\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    955\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer.lower().endswith(\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[32m    959\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    962\u001b[39m     warnings.warn(\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal json to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    968\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File data/raw/ticketmaster_us_music_latest.json does not exist"
     ]
    }
   ],
   "source": [
    "enriched_df = build_weather_event_panel(\n",
    "    event_json_path=\"data/raw/ticketmaster_us_music_latest.json\",\n",
    "    out_path=\"data/processed/event_weather_panel.parquet\",\n",
    ")\n",
    "\n",
    "enriched_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
