{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c05c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 17:15:42,214 - INFO - Starting data conversion in directory: raw/json\n",
      "2025-12-07 17:15:42,215 - INFO - Processing file: raw/json/ticketmaster_events_20251207_023443.json\n",
      "2025-12-07 17:15:42,341 - ERROR - Error processing file raw/json/ticketmaster_events_20251207_023443.json: Expected object or value\n",
      "2025-12-07 17:15:42,356 - INFO - Processing file: raw/json/ticketmaster_us_music_20251207_031136.json\n",
      "2025-12-07 17:15:42,502 - ERROR - Error processing file raw/json/ticketmaster_us_music_20251207_031136.json: Expected object or value\n",
      "2025-12-07 17:15:42,514 - INFO - Conversion complete. Total files processed: **0**.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "# The folder containing your raw data files\n",
    "RAW_DATA_DIR = 'raw/json' \n",
    "# Supported file extensions to convert FROM\n",
    "SUPPORTED_INPUT_EXTENSIONS = ['*.parquet', '*.json']\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def convert_files_to_csv():\n",
    "    \"\"\"\n",
    "    Iterates through the RAW_DATA_DIR, converts all supported files (parquet, json)\n",
    "    to CSV format, and saves the new CSV files in the same directory.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting data conversion in directory: {RAW_DATA_DIR}\")\n",
    "\n",
    "    if not os.path.isdir(RAW_DATA_DIR):\n",
    "        logging.error(f\"Directory not found: {RAW_DATA_DIR}. Please create it or adjust the RAW_DATA_DIR variable.\")\n",
    "        return\n",
    "\n",
    "    # List to track all processed files\n",
    "    processed_files_count = 0\n",
    "    \n",
    "    # 1. Iterate over all supported extensions\n",
    "    for ext_pattern in SUPPORTED_INPUT_EXTENSIONS:\n",
    "        # Create the full path pattern for glob (e.g., 'data/raw/*.parquet')\n",
    "        search_path = os.path.join(RAW_DATA_DIR, ext_pattern)\n",
    "        \n",
    "        # Use glob.glob to find all matching files\n",
    "        for input_filepath in glob.glob(search_path):\n",
    "            try:\n",
    "                # Get the base name without the original extension\n",
    "                base_name = os.path.splitext(os.path.basename(input_filepath))[0]\n",
    "                # Define the new output CSV file path\n",
    "                output_filepath = os.path.join(RAW_DATA_DIR, f\"{base_name}.csv\")\n",
    "\n",
    "                logging.info(f\"Processing file: {input_filepath}\")\n",
    "\n",
    "                # 2. Read the file into a Pandas DataFrame\n",
    "                if ext_pattern == '*.parquet':\n",
    "                    df = pd.read_parquet(input_filepath)\n",
    "                elif ext_pattern == '*.json':\n",
    "                    # Assuming standard line-delimited or simple JSON structure\n",
    "                    # Adjust 'lines=True' if your JSON is a single object array\n",
    "                    df = pd.read_json(input_filepath, lines=True) \n",
    "                else:\n",
    "                    # This case should ideally not be reached\n",
    "                    continue\n",
    "\n",
    "                # 3. Write the DataFrame to a CSV file\n",
    "                # index=False prevents writing the DataFrame's index as a column\n",
    "                df.to_csv(output_filepath, index=False)\n",
    "                \n",
    "                logging.info(f\"Successfully converted to: {output_filepath}\")\n",
    "                processed_files_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {input_filepath}: {e}\")\n",
    "\n",
    "    logging.info(f\"Conversion complete. Total files processed: **{processed_files_count}**.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    convert_files_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850b3f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 17:20:43,868 - INFO - Starting data conversion in directory: raw/json\n",
      "2025-12-07 17:20:43,875 - INFO - Processing file: raw/json/ticketmaster_events_20251207_023443.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 17:20:44,448 - INFO - Successfully converted to: raw/json/ticketmaster_events_20251207_023443.csv (Rows: 1000)\n",
      "2025-12-07 17:20:44,448 - INFO - Processing file: raw/json/ticketmaster_us_music_20251207_031136.json\n",
      "2025-12-07 17:20:44,999 - INFO - Successfully converted to: raw/json/ticketmaster_us_music_20251207_031136.csv (Rows: 16050)\n",
      "2025-12-07 17:20:45,000 - INFO - Conversion complete. Total files processed: **2**.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import json\n",
    "from pandas import json_normalize # json_normalize is now a top-level function\n",
    "\n",
    "# --- Configuration ---\n",
    "# The folder containing your raw data files\n",
    "RAW_DATA_DIR = 'raw/json' \n",
    "# Supported file extensions to convert FROM\n",
    "SUPPORTED_INPUT_EXTENSIONS = ['*.parquet', '*.json']\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def convert_files_to_csv():\n",
    "    \"\"\"\n",
    "    Iterates through the RAW_DATA_DIR, converts all supported files (parquet, json)\n",
    "    to CSV format, and saves the new CSV files in the same directory.\n",
    "    \n",
    "    Includes robust logic for nested JSON files common with API outputs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting data conversion in directory: {RAW_DATA_DIR}\")\n",
    "\n",
    "    if not os.path.isdir(RAW_DATA_DIR):\n",
    "        logging.error(f\"Directory not found: {RAW_DATA_DIR}. Please create it or adjust the RAW_DATA_DIR variable.\")\n",
    "        return\n",
    "\n",
    "    # List to track all processed files\n",
    "    processed_files_count = 0\n",
    "    \n",
    "    # 1. Iterate over all supported extensions\n",
    "    for ext_pattern in SUPPORTED_INPUT_EXTENSIONS:\n",
    "        # Create the full path pattern for glob (e.g., 'data/raw/*.parquet')\n",
    "        search_path = os.path.join(RAW_DATA_DIR, ext_pattern)\n",
    "        \n",
    "        # Use glob.glob to find all matching files\n",
    "        for input_filepath in glob.glob(search_path):\n",
    "            try:\n",
    "                # Get the base name without the original extension\n",
    "                base_name = os.path.splitext(os.path.basename(input_filepath))[0]\n",
    "                # Define the new output CSV file path\n",
    "                output_filepath = os.path.join(RAW_DATA_DIR, f\"{base_name}.csv\")\n",
    "\n",
    "                logging.info(f\"Processing file: {input_filepath}\")\n",
    "\n",
    "                # 2. Read the file into a Pandas DataFrame\n",
    "                if ext_pattern == '*.parquet':\n",
    "                    # Read Parquet file\n",
    "                    df = pd.read_parquet(input_filepath)\n",
    "\n",
    "                elif ext_pattern == '*.json':\n",
    "                    # Read JSON file with robust nested handling\n",
    "                    with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    # --- ADVANCED JSON PARSING LOGIC ---\n",
    "                    data_to_normalize = None\n",
    "                    \n",
    "                    if isinstance(data, list):\n",
    "                        # Case 1: The root object is already a list of records\n",
    "                        data_to_normalize = data\n",
    "                    elif '_embedded' in data and 'events' in data['_embedded']:\n",
    "                        # Case 2: Common Ticketmaster structure\n",
    "                        data_to_normalize = data['_embedded']['events']\n",
    "                    elif 'events' in data:\n",
    "                        # Case 3: Data is directly under an 'events' key\n",
    "                        data_to_normalize = data['events']\n",
    "                    else:\n",
    "                        # Case 4: Fallback to try and normalize the root dictionary\n",
    "                        data_to_normalize = data\n",
    "\n",
    "                    if data_to_normalize is None or (isinstance(data_to_normalize, list) and not data_to_normalize):\n",
    "                         # Handle case where the list is empty or the key was not found\n",
    "                         logging.warning(f\"  -> Skipped: Could not find array of records to flatten in {base_name}. JSON may be empty or nested differently.\")\n",
    "                         continue\n",
    "                        \n",
    "                    # Flatten the JSON structure, expanding nested dictionaries into columns (e.g., '_embedded.events.venue.name')\n",
    "                    # Use errors='ignore' to skip non-list/dict values if the key is not strictly a list\n",
    "                    df = json_normalize(data_to_normalize, errors='ignore')\n",
    "\n",
    "                else:\n",
    "                    # This case should ideally not be reached\n",
    "                    continue\n",
    "\n",
    "                # 3. Write the DataFrame to a CSV file\n",
    "                # index=False prevents writing the DataFrame's index as a column\n",
    "                df.to_csv(output_filepath, index=False)\n",
    "                \n",
    "                logging.info(f\"Successfully converted to: {output_filepath} (Rows: {len(df)})\")\n",
    "                processed_files_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {input_filepath}: {e}\")\n",
    "\n",
    "    logging.info(f\"Conversion complete. Total files processed: **{processed_files_count}**.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: Requires 'pip install pandas pyarrow' \n",
    "    convert_files_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7d8503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parquet File Size and Integrity Check ---\n",
      "\n",
      "File: **setlistfm_us_concerts_20251207_080243.parquet**\n",
      "  -> Disk Size: 0.23 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **predicthq_events_20251207_054022.parquet**\n",
      "  -> Disk Size: 0.00 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **phq_events_full_20251207_060912.parquet**\n",
      "  -> Disk Size: 2.23 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **setlistfm_us_concerts_20251207_080539.parquet**\n",
      "  -> Disk Size: 0.42 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **phq_events_20251207_054509.parquet**\n",
      "  -> Disk Size: 0.03 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **phq_events_full_20251207_065623.parquet**\n",
      "  -> Disk Size: 2.23 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **setlistfm_us_concerts_20251207_074105.parquet**\n",
      "  -> Disk Size: 0.01 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **musicbrainz_structured_features.parquet**\n",
      "  -> Disk Size: 5.30 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **phq_events_20251207_054229.parquet**\n",
      "  -> Disk Size: 0.01 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n",
      "\n",
      "File: **phq_events_full_20251207_063822.parquet**\n",
      "  -> Disk Size: 2.23 MB\n",
      "  -> Row Count: 0\n",
      "  -> WARNING: File is empty (0 rows).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = 'raw/parquet' \n",
    "PARQUET_PATTERN = os.path.join(DATA_DIR, '*.parquet')\n",
    "\n",
    "print(\"--- Parquet File Size and Integrity Check ---\")\n",
    "\n",
    "parquet_files = glob.glob(PARQUET_PATTERN)\n",
    "\n",
    "if not parquet_files:\n",
    "    print(f\"No .parquet files found in {DATA_DIR}.\")\n",
    "else:\n",
    "    for filepath in parquet_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        \n",
    "        # 1. Get file size from the OS\n",
    "        file_size_bytes = os.path.getsize(filepath)\n",
    "        file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "        \n",
    "        # 2. Check data integrity and record count without loading the full DataFrame\n",
    "        try:\n",
    "            # Use 'engine='pyarrow'' for potentially faster metadata reading\n",
    "            # This loads only the metadata/schema, not the full data.\n",
    "            metadata = pd.read_parquet(filepath, engine='pyarrow', columns=[]).index\n",
    "            \n",
    "            # The length of the index will give the row count\n",
    "            row_count = len(metadata)\n",
    "            \n",
    "            print(f\"\\nFile: **{filename}**\")\n",
    "            print(f\"  -> Disk Size: {file_size_mb:.2f} MB\")\n",
    "            print(f\"  -> Row Count: {row_count:,}\")\n",
    "            \n",
    "            if row_count == 0:\n",
    "                print(\"  -> WARNING: File is empty (0 rows).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFile: **{filename}**\")\n",
    "            print(f\"  -> ERROR: Failed to read metadata. File may be corrupted. Details: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
